{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>no_chars</th>\n",
       "      <th>no_words</th>\n",
       "      <th>no_unique_words</th>\n",
       "      <th>word_difference</th>\n",
       "      <th>number_hashtags</th>\n",
       "      <th>number_mentions</th>\n",
       "      <th>number_urls</th>\n",
       "      <th>client_sources</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>geo_enabled</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>user_lang</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#DanAndPhilBRITs Make sure they zoom in on Lou...</td>\n",
       "      <td>4.499810</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5604</td>\n",
       "      <td>259</td>\n",
       "      <td>4880</td>\n",
       "      <td>nl</td>\n",
       "      <td>446</td>\n",
       "      <td>reg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I voted for DRAG ME DOWN by @onedirection for ...</td>\n",
       "      <td>4.828314</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>Mobile Web</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>198</td>\n",
       "      <td>64</td>\n",
       "      <td>370</td>\n",
       "      <td>en</td>\n",
       "      <td>380</td>\n",
       "      <td>cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I voted for DRAG ME DOWN by @onedirection for ...</td>\n",
       "      <td>4.867534</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3843</td>\n",
       "      <td>794</td>\n",
       "      <td>2401</td>\n",
       "      <td>en</td>\n",
       "      <td>334</td>\n",
       "      <td>cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@brits @danisnotonfire @AmazingPhil #BRITs #Da...</td>\n",
       "      <td>4.304065</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>24432</td>\n",
       "      <td>3608</td>\n",
       "      <td>29325</td>\n",
       "      <td>en</td>\n",
       "      <td>2974</td>\n",
       "      <td>cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello from RUSSIA &lt;ed&gt;&lt;U+00A0&gt;&lt;U+00BC&gt;&lt;ed&gt;&lt;U+0...</td>\n",
       "      <td>5.863631</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3274</td>\n",
       "      <td>45</td>\n",
       "      <td>31</td>\n",
       "      <td>ru</td>\n",
       "      <td>21</td>\n",
       "      <td>cont</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  no_chars  no_words  \\\n",
       "0  #DanAndPhilBRITs Make sure they zoom in on Lou...  4.499810  2.639057   \n",
       "1  I voted for DRAG ME DOWN by @onedirection for ...  4.828314  2.890372   \n",
       "2  I voted for DRAG ME DOWN by @onedirection for ...  4.867534  2.995732   \n",
       "3  @brits @danisnotonfire @AmazingPhil #BRITs #Da...  4.304065  2.079442   \n",
       "4  Hello from RUSSIA <ed><U+00A0><U+00BC><ed><U+0...  5.863631  2.708050   \n",
       "\n",
       "   no_unique_words  word_difference  number_hashtags  number_mentions  \\\n",
       "0         2.639057         0.000000         1.098612         0.000000   \n",
       "1         2.833213         0.693147         1.098612         0.693147   \n",
       "2         2.944439         0.693147         1.098612         0.693147   \n",
       "3         2.079442         0.000000         1.098612         1.386294   \n",
       "4         2.708050         0.000000         1.098612         1.098612   \n",
       "\n",
       "   number_urls      client_sources  listed_count  verified  geo_enabled  \\\n",
       "0     0.000000  Twitter for iPhone             4     False         True   \n",
       "1     0.693147          Mobile Web             0     False        False   \n",
       "2     0.693147  Twitter Web Client             4     False        False   \n",
       "3     0.000000  Twitter for iPhone             9     False         True   \n",
       "4     0.000000  Twitter for iPhone             3     False        False   \n",
       "\n",
       "   statuses_count  followers_count  favourites_count user_lang  friends_count  \\\n",
       "0            5604              259              4880        nl            446   \n",
       "1             198               64               370        en            380   \n",
       "2            3843              794              2401        en            334   \n",
       "3           24432             3608             29325        en           2974   \n",
       "4            3274               45                31        ru             21   \n",
       "\n",
       "  class  \n",
       "0   reg  \n",
       "1  cont  \n",
       "2  cont  \n",
       "3  cont  \n",
       "4  cont  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data = pd.read_csv(\"text_quality.csv\", encoding = \"ISO-8859-1\")\n",
    "tweet_data = tweet_data.drop_duplicates(subset=\"text\")\n",
    "\n",
    "tweet_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Tweets %:  74.8\n",
      "Contaminated Tweets %:  25.2\n"
     ]
    }
   ],
   "source": [
    "# Proportion of classes\n",
    "\n",
    "reg_class = tweet_data[(tweet_data[\"class\"] == \"reg\")]\n",
    "cont_class = tweet_data[(tweet_data[\"class\"] == 'cont')]\n",
    "\n",
    "# tweet_data['class'] = tweet_data['class'].map({'reg': [1, 0], 'cont': [0, 1]})\n",
    "# tweet_data['class'] = tweet_data['class'].map({'reg': 2, 'cont': 1})\n",
    "tweet_data['class'] = tweet_data['class'].map({'reg': 1, 'cont': 0})\n",
    "\n",
    "print(\"Regular Tweets %: \", round((len(reg_class) / (len(reg_class) + len(cont_class))) * 100, 2))\n",
    "print(\"Contaminated Tweets %: \", round((len(cont_class) / (len(reg_class) + len(cont_class))) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = tweet_data.drop([\"text\",\"client_sources\",\"user_lang\",\"class\"], axis=1).as_matrix() #np.float32()\n",
    "outputData = tweet_data[\"class\"].as_matrix()\n",
    "\n",
    "# trainloader_input = torch.utils.data.DataLoader(inputData, batch_size=4, shuffle=True, num_workers=8)\n",
    "# trainloader_output = torch.utils.data.DataLoader(outputData, batch_size=4, shuffle=True, num_workers=8)\n",
    "# print(trainloader_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example NN using torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 1, inputData.shape[1], 8, 2\n",
    "\n",
    "# Set our variables up\n",
    "x_data = inputData # Variable(torch.from_numpy(inputData))\n",
    "y_data = outputData # Variable(torch.from_numpy(outputData), requires_grad=False)\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.897\n",
      "[1,  4000] loss: 0.528\n",
      "[1,  6000] loss: 0.525\n",
      "[1,  8000] loss: 0.518\n",
      "[1, 10000] loss: 0.520\n",
      "[1, 12000] loss: 0.695\n",
      "[1, 14000] loss: 0.552\n",
      "[1, 16000] loss: 0.511\n",
      "[1, 18000] loss: 0.549\n",
      "[1, 20000] loss: 0.513\n",
      "[1, 22000] loss: 0.648\n",
      "[1, 24000] loss: 0.472\n",
      "[1, 26000] loss: 0.430\n",
      "[1, 28000] loss: 0.504\n",
      "[1, 30000] loss: 0.526\n",
      "[1, 32000] loss: 0.429\n",
      "[1, 34000] loss: 0.513\n",
      "[2,  2000] loss: 0.641\n",
      "[2,  4000] loss: 0.528\n",
      "[2,  6000] loss: 0.525\n",
      "[2,  8000] loss: 0.518\n",
      "[2, 10000] loss: 0.520\n",
      "[2, 12000] loss: 0.695\n",
      "[2, 14000] loss: 0.552\n",
      "[2, 16000] loss: 0.511\n",
      "[2, 18000] loss: 0.549\n",
      "[2, 20000] loss: 0.513\n",
      "[2, 22000] loss: 0.648\n",
      "[2, 24000] loss: 0.472\n",
      "[2, 26000] loss: 0.430\n",
      "[2, 28000] loss: 0.504\n",
      "[2, 30000] loss: 0.526\n",
      "[2, 32000] loss: 0.429\n",
      "[2, 34000] loss: 0.513\n",
      "[3,  2000] loss: 0.641\n",
      "[3,  4000] loss: 0.528\n",
      "[3,  6000] loss: 0.525\n",
      "[3,  8000] loss: 0.518\n",
      "[3, 10000] loss: 0.520\n",
      "[3, 12000] loss: 0.695\n",
      "[3, 14000] loss: 0.552\n",
      "[3, 16000] loss: 0.511\n",
      "[3, 18000] loss: 0.549\n",
      "[3, 20000] loss: 0.513\n",
      "[3, 22000] loss: 0.648\n",
      "[3, 24000] loss: 0.472\n",
      "[3, 26000] loss: 0.430\n",
      "[3, 28000] loss: 0.504\n",
      "[3, 30000] loss: 0.526\n",
      "[3, 32000] loss: 0.429\n",
      "[3, 34000] loss: 0.513\n",
      "[4,  2000] loss: 0.641\n",
      "[4,  4000] loss: 0.528\n",
      "[4,  6000] loss: 0.525\n",
      "[4,  8000] loss: 0.518\n",
      "[4, 10000] loss: 0.520\n",
      "[4, 12000] loss: 0.695\n",
      "[4, 14000] loss: 0.552\n",
      "[4, 16000] loss: 0.511\n",
      "[4, 18000] loss: 0.549\n",
      "[4, 20000] loss: 0.513\n",
      "[4, 22000] loss: 0.648\n",
      "[4, 24000] loss: 0.472\n",
      "[4, 26000] loss: 0.430\n",
      "[4, 28000] loss: 0.504\n",
      "[4, 30000] loss: 0.526\n",
      "[4, 32000] loss: 0.429\n",
      "[4, 34000] loss: 0.513\n",
      "[5,  2000] loss: 0.641\n",
      "[5,  4000] loss: 0.528\n",
      "[5,  6000] loss: 0.525\n",
      "[5,  8000] loss: 0.518\n",
      "[5, 10000] loss: 0.520\n",
      "[5, 12000] loss: 0.695\n",
      "[5, 14000] loss: 0.552\n",
      "[5, 16000] loss: 0.511\n",
      "[5, 18000] loss: 0.549\n",
      "[5, 20000] loss: 0.513\n",
      "[5, 22000] loss: 0.648\n",
      "[5, 24000] loss: 0.472\n",
      "[5, 26000] loss: 0.430\n",
      "[5, 28000] loss: 0.504\n",
      "[5, 30000] loss: 0.526\n",
      "[5, 32000] loss: 0.429\n",
      "[5, 34000] loss: 0.513\n",
      "[6,  2000] loss: 0.641\n",
      "[6,  4000] loss: 0.528\n",
      "[6,  6000] loss: 0.525\n",
      "[6,  8000] loss: 0.518\n",
      "[6, 10000] loss: 0.520\n",
      "[6, 12000] loss: 0.695\n",
      "[6, 14000] loss: 0.552\n",
      "[6, 16000] loss: 0.511\n",
      "[6, 18000] loss: 0.549\n",
      "[6, 20000] loss: 0.513\n",
      "[6, 22000] loss: 0.648\n",
      "[6, 24000] loss: 0.472\n",
      "[6, 26000] loss: 0.430\n",
      "[6, 28000] loss: 0.504\n",
      "[6, 30000] loss: 0.526\n",
      "[6, 32000] loss: 0.429\n",
      "[6, 34000] loss: 0.513\n",
      "[7,  2000] loss: 0.641\n",
      "[7,  4000] loss: 0.528\n",
      "[7,  6000] loss: 0.525\n",
      "[7,  8000] loss: 0.518\n",
      "[7, 10000] loss: 0.520\n",
      "[7, 12000] loss: 0.695\n",
      "[7, 14000] loss: 0.552\n",
      "[7, 16000] loss: 0.511\n",
      "[7, 18000] loss: 0.549\n",
      "[7, 20000] loss: 0.513\n",
      "[7, 22000] loss: 0.648\n",
      "[7, 24000] loss: 0.472\n",
      "[7, 26000] loss: 0.430\n",
      "[7, 28000] loss: 0.504\n",
      "[7, 30000] loss: 0.526\n",
      "[7, 32000] loss: 0.429\n",
      "[7, 34000] loss: 0.513\n",
      "[8,  2000] loss: 0.641\n",
      "[8,  4000] loss: 0.528\n",
      "[8,  6000] loss: 0.525\n",
      "[8,  8000] loss: 0.518\n",
      "[8, 10000] loss: 0.520\n",
      "[8, 12000] loss: 0.695\n",
      "[8, 14000] loss: 0.552\n",
      "[8, 16000] loss: 0.511\n",
      "[8, 18000] loss: 0.549\n",
      "[8, 20000] loss: 0.513\n",
      "[8, 22000] loss: 0.648\n",
      "[8, 24000] loss: 0.472\n",
      "[8, 26000] loss: 0.430\n",
      "[8, 28000] loss: 0.504\n",
      "[8, 30000] loss: 0.526\n",
      "[8, 32000] loss: 0.429\n",
      "[8, 34000] loss: 0.513\n",
      "[9,  2000] loss: 0.641\n",
      "[9,  4000] loss: 0.528\n",
      "[9,  6000] loss: 0.525\n",
      "[9,  8000] loss: 0.518\n",
      "[9, 10000] loss: 0.520\n",
      "[9, 12000] loss: 0.695\n",
      "[9, 14000] loss: 0.552\n",
      "[9, 16000] loss: 0.511\n",
      "[9, 18000] loss: 0.549\n",
      "[9, 20000] loss: 0.513\n",
      "[9, 22000] loss: 0.648\n",
      "[9, 24000] loss: 0.472\n",
      "[9, 26000] loss: 0.430\n",
      "[9, 28000] loss: 0.504\n",
      "[9, 30000] loss: 0.526\n",
      "[9, 32000] loss: 0.429\n",
      "[9, 34000] loss: 0.513\n",
      "[10,  2000] loss: 0.641\n",
      "[10,  4000] loss: 0.528\n",
      "[10,  6000] loss: 0.525\n",
      "[10,  8000] loss: 0.518\n",
      "[10, 10000] loss: 0.520\n",
      "[10, 12000] loss: 0.695\n",
      "[10, 14000] loss: 0.552\n",
      "[10, 16000] loss: 0.511\n",
      "[10, 18000] loss: 0.549\n",
      "[10, 20000] loss: 0.513\n",
      "[10, 22000] loss: 0.648\n",
      "[10, 24000] loss: 0.472\n",
      "[10, 26000] loss: 0.430\n",
      "[10, 28000] loss: 0.504\n",
      "[10, 30000] loss: 0.526\n",
      "[10, 32000] loss: 0.429\n",
      "[10, 34000] loss: 0.513\n"
     ]
    }
   ],
   "source": [
    "# Optimize and error calculation\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for t in range(10):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(zip(x_data, y_data), 0):\n",
    "    #for i, data in enumerate(zip(trainloader_input, trainloader_output), 0):\n",
    "        \n",
    "        x, y = data\n",
    "#         x = Variable(x)\n",
    "#         y = Variable(torch.stack(y, 0))\n",
    "#         x = np.long32(x).tolist()\n",
    "        \n",
    "#         x = Variable(torch.from_numpy(x))\n",
    "        \n",
    "        x = Variable(torch.FloatTensor([x]))\n",
    "        y = Variable(torch.IntTensor([int(y)]))\n",
    "        #y = Variable(torch.FloatTensor(y)) #[int(y)]))\n",
    "        \n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y.long())\n",
    "#         print(t, loss.data[0])\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (t + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.5235109329223633\n",
      "1 2.334049701690674\n",
      "2 2.165907144546509\n",
      "3 2.0116193294525146\n",
      "4 1.876344084739685\n",
      "5 1.7554758787155151\n",
      "6 1.6425457000732422\n",
      "7 1.537375807762146\n",
      "8 1.4427156448364258\n",
      "9 1.3539345264434814\n",
      "10 1.271077036857605\n",
      "11 1.1933000087738037\n",
      "12 1.1202627420425415\n",
      "13 1.0524216890335083\n",
      "14 0.9896336793899536\n",
      "15 0.9316871166229248\n",
      "16 0.8776638507843018\n",
      "17 0.8269673585891724\n",
      "18 0.7791957259178162\n",
      "19 0.7341635227203369\n",
      "20 0.6917116045951843\n",
      "21 0.6516913175582886\n",
      "22 0.6139622926712036\n",
      "23 0.5783936381340027\n",
      "24 0.5448616743087769\n",
      "25 0.5132501721382141\n",
      "26 0.48345011472702026\n",
      "27 0.4553581178188324\n",
      "28 0.4288780689239502\n",
      "29 0.40391805768013\n",
      "30 0.38039201498031616\n",
      "31 0.3582189977169037\n",
      "32 0.3373225927352905\n",
      "33 0.3184129297733307\n",
      "34 0.3018184006214142\n",
      "35 0.28607842326164246\n",
      "36 0.27114975452423096\n",
      "37 0.25699126720428467\n",
      "38 0.24356409907341003\n",
      "39 0.23083099722862244\n",
      "40 0.2187567949295044\n",
      "41 0.20751559734344482\n",
      "42 0.19706718623638153\n",
      "43 0.18713364005088806\n",
      "44 0.17769041657447815\n",
      "45 0.16871404647827148\n",
      "46 0.16018229722976685\n",
      "47 0.15208525955677032\n",
      "48 0.14464065432548523\n",
      "49 0.13755103945732117\n",
      "50 0.1308000683784485\n",
      "51 0.12466471642255783\n",
      "52 0.11881518363952637\n",
      "53 0.11338914930820465\n",
      "54 0.10820522904396057\n",
      "55 0.10325316339731216\n",
      "56 0.0985228568315506\n",
      "57 0.0940048098564148\n",
      "58 0.0896897166967392\n",
      "59 0.08556894212961197\n",
      "60 0.0816337987780571\n",
      "61 0.07787634432315826\n",
      "62 0.07428885251283646\n",
      "63 0.0708637535572052\n",
      "64 0.06759388744831085\n",
      "65 0.06447258591651917\n",
      "66 0.061493221670389175\n",
      "67 0.058649420738220215\n",
      "68 0.05593538656830788\n",
      "69 0.05334511771798134\n",
      "70 0.05087326467037201\n",
      "71 0.04851451516151428\n",
      "72 0.04626377671957016\n",
      "73 0.04411628469824791\n",
      "74 0.04206739366054535\n",
      "75 0.040112610906362534\n",
      "76 0.03824780881404877\n",
      "77 0.03646880388259888\n",
      "78 0.03477182611823082\n",
      "79 0.03315311670303345\n",
      "80 0.03160915523767471\n",
      "81 0.03013659454882145\n",
      "82 0.028732042759656906\n",
      "83 0.027392543852329254\n",
      "84 0.026115059852600098\n",
      "85 0.024896830320358276\n",
      "86 0.023735105991363525\n",
      "87 0.02262726239860058\n",
      "88 0.021570909768342972\n",
      "89 0.020563630387187004\n",
      "90 0.019603200256824493\n",
      "91 0.018687475472688675\n",
      "92 0.017814312130212784\n",
      "93 0.016981901600956917\n",
      "94 0.016188204288482666\n",
      "95 0.01543155312538147\n",
      "96 0.01471018884330988\n",
      "97 0.014022482559084892\n",
      "98 0.01336684450507164\n",
      "99 0.01274184137582779\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 1, 1000, 100, 2\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(100):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "already counted a million dimensions in a given sequence. Most likely your items are also sequences and there's no way to infer how many dimension should the tensor have",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-4ff6f260e47b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: already counted a million dimensions in a given sequence. Most likely your items are also sequences and there's no way to infer how many dimension should the tensor have"
     ]
    }
   ],
   "source": [
    "torch.FloatTensor([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
